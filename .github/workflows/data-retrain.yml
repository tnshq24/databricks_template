name: Data-Driven Retraining

on:
  schedule:
    - cron: '0 */6 * * *'  # Every 6 hours
  repository_dispatch:
    types: [new-data-available]
  workflow_dispatch:  # Manual trigger
  workflow_run:
    workflows: ["MLOps Pipeline"]
    types: [completed]

env:
  DATABRICKS_HOST: https://adb-1244961191947049.9.azuredatabricks.net

jobs:
  check-and-retrain:
    name: Check Data and Retrain if Needed
    runs-on: ubuntu-latest
    if: github.event_name != 'workflow_run' || github.event.workflow_run.conclusion == 'success'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install databricks-cli databricks-sdk
        
    - name: Configure Databricks CLI
      run: |
        echo "[DEFAULT]" > ~/.databrickscfg
        echo "host = ${{ env.DATABRICKS_HOST }}" >> ~/.databrickscfg
        echo "token = ${{ secrets.DATABRICKS_TOKEN }}" >> ~/.databrickscfg

    - name: Verify Databricks Connection
      run: |
        databricks workspace list /Shared --output json | head -5
        echo "‚úÖ Databricks CLI configured successfully"

    - name: Check for Auto-Retraining Job
      id: check_job
      run: |
        # Find the auto-retraining job ID
        echo "üîç Searching for auto-retraining job..."
        JOB_LIST=$(databricks jobs list --output json)
        JOB_ID=$(echo "$JOB_LIST" | jq -r '.jobs[] | select(.settings.name | contains("Auto-Retraining-Monitor")) | .job_id' | head -1)
        
        if [ -z "$JOB_ID" ] || [ "$JOB_ID" = "null" ]; then
          echo "‚ùå Auto-retraining job not found."
          echo "üìã Available jobs:"
          echo "$JOB_LIST" | jq -r '.jobs[] | "\(.job_id): \(.settings.name)"'
          echo ""
          echo "üí° This usually means:"
          echo "  1. The main MLOps pipeline hasn't run successfully yet"
          echo "  2. The auto-retraining job creation failed"
          echo "  3. The job name doesn't match exactly"
          echo ""
          echo "üîß Please run the main MLOps pipeline first to create the auto-retraining job."
          exit 1
        fi
        
        echo "job_id=$JOB_ID" >> $GITHUB_OUTPUT
        echo "‚úÖ Found auto-retraining job with ID: $JOB_ID"

    - name: Trigger Data Monitoring and Retraining
      run: |
        JOB_ID="${{ steps.check_job.outputs.job_id }}"
        
        # Start the data monitoring job
        echo "üîç Starting data monitoring and retraining check..."
        RUN_ID=$(databricks jobs run-now --job-id $JOB_ID | jq -r .run_id)
        echo "üöÄ Started monitoring run with ID: $RUN_ID"
        
        # Wait for completion (with shorter timeout for monitoring)
        timeout=1800  # 30 minutes
        elapsed=0
        interval=30
        
        while [ $elapsed -lt $timeout ]; do
          STATUS=$(databricks runs get --run-id $RUN_ID | jq -r .state.life_cycle_state)
          echo "‚è≥ Monitoring job status: $STATUS (${elapsed}s elapsed)"
          
          if [ "$STATUS" = "TERMINATED" ]; then
            RESULT=$(databricks runs get --run-id $RUN_ID | jq -r .state.result_state)
            echo "üèÅ Monitoring job result: $RESULT"
            
            if [ "$RESULT" = "SUCCESS" ]; then
              echo "‚úÖ Data monitoring completed successfully!"
              
              # Check if retraining was triggered
              LOG_OUTPUT=$(databricks runs get-output --run-id $RUN_ID | jq -r .logs)
              if echo "$LOG_OUTPUT" | grep -q "Triggering automatic model retraining"; then
                echo "üîÑ Model retraining was triggered due to data drift or performance issues"
              else
                echo "‚ÑπÔ∏è No retraining needed - model is performing well"
              fi
              
              exit 0
            else
              echo "‚ùå Data monitoring failed!"
              exit 1
            fi
          fi
          
          sleep $interval
          elapsed=$((elapsed + interval))
        done
        
        echo "‚è∞ Data monitoring job timed out"
        exit 1

    - name: Notify Result
      if: always()
      run: |
        if [ "${{ job.status }}" = "success" ]; then
          echo "‚úÖ Data monitoring completed successfully"
          echo "üìä Check Databricks for detailed results"
        else
          echo "‚ùå Data monitoring failed"
          echo "üîç Check the logs above for details"
        fi
        
        echo ""
        echo "üåê Monitor your pipeline:"
        echo "  - Databricks Jobs: ${{ env.DATABRICKS_HOST }}/#job/list"
        echo "  - MLflow Experiments: ${{ env.DATABRICKS_HOST }}/#mlflow/experiments"
        echo "  - Model Serving: ${{ env.DATABRICKS_HOST }}/#/serving-endpoints" 